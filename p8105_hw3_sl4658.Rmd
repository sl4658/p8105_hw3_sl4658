---
title: "p8105_hw3_sl4658"
author: "Simin Ling"
date: "10/9/2020"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


# Problem 1
```{r}
data("instacart")
head(instacart)
summary(instacart)
```

### Description of the instacart dataset 

There are `r nrow(instacart)` observations in the instacart dataset, with `r nrow(instacart)` rows and `r ncol(instacart)` columns. Each observations are the level of items in orders by user, which correspond to order-related information for each product in each order placed by each user. 

Some key variables are user/order related variables, including order id, user id, product id and name, count of product in each order, order date, reordered or no, aisle id and name, department and department id, and more. For example, an user (user id of 112108) placed an order (order id of 1) that included one Bulgarian Yogurt (product id of 49302) as a reordered item, in the dairy eggs department and yogurt aisle (department id of 16 and aisle id of 120).


### Answers to the questions in Problem 1
Problem 1a. 
```{r}
instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```
There are 134 aisles in the dataset, and the fresh vegetables aisles are the most items ordered from with an observation count of 150609.


Problem 1b.
```{r}
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>%
  ggplot(aes(x = aisle, y = n, color = aisle)) + geom_point(stat="identity",show.legend = F) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

As shown in the graph, the fresh vegetables and fresh fruits aisles are the ones that have most items sold. The aisles are listed from left to right in an ascending order of item sold.


Problem 1c.
```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>%
  filter(min_rank(desc(n)) < 4) %>% 
  arrange(aisle, rank) %>%
  knitr::kable()
```
The three most popular items in the baking ingredients aisle are light brown sugar (n=499), pure baking soda (n=387), and can sugar (n=336), in descending order.
The three most popular items in the dog food care aisle are Snack Sticks Chicken & Rice Recipe Dog Treats (n=30), Organix Chicken & Brown Rice Recipe (n=28), and Small Dog Biscuits (n=26), in descending order.
The three most popular items in the packaged vegetables fruits aisle are Organic Baby Spinach (n=9784), Organic Raspberries (n=5546), and Organic Blueberries (n=4966), in descending order.


Problem 1d.
```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  mutate(order_dow = recode(order_dow, `0`="Sun", `1`="Mon", `2`="Tue", `3`="Wed", `4`="Thur", `5`="Fri", `6`="Sat")) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```
The mean hour of the day at which Pink Lady Apples are ordered are generally earlier than the mean hour of the day at which Coffee Ice Cream are ordered, except for Friday.


# Problem 2

## Load, tidy, and otherwise wrangle the data.
```{r, warning=FALSE, message=FALSE}
accel_df = read_csv("~/Desktop/Fall 2020/Data Science/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minutes",
    names_prefix = "activity_",
    values_to = "activity_count"
    ) %>%
  mutate(day_type = ifelse(day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"), "Weekday", "Weekend")) %>%
  mutate(
    day = as.factor(day),
    minutes = as.numeric(minutes),
    day_type = as.factor(day_type),
    day = forcats::fct_relevel(day,c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"))
  ) 
```

After tidying the data, there are `r nrow(accel_df)` rows/observations and `r ncol(accel_df)` columns in the resulting dataset. The variables in the dataset include information on the week, day (day id, which day, and the type of day), and minute when the activity count measurement took place, as well as the measured outcome of activity counts in a short period. The variable day describes if the measurement was taken on Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, or Sunday; and the variable day_type describes if the measurement was taken on weekday or weekend.


## Using the tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals.
```{r}
accel_df %>% 
  group_by(week, day) %>% 
  summarize(total_activity = sum(activity_count)) %>%
  pivot_wider(
    names_from = day,
    values_from = total_activity
  )
```

There is no exact trend observed from the table, as the patient had ups and downs in the activity count among days and weeks, which led to different trends that contradict with each other. Despite that the patient had low activity count on the Saturdays of week 4 and 5, we still need further analysis to get an observed trend. 


## Make a single panel plot that shows the 24-hour activity time courses for each day.
```{r}
accel_df %>%
ggplot(aes(x = minutes, y = activity_count, color = day)) + geom_line(aes(group = day), alpha = 0.5) + 
    labs(
      title = "24-hour Activity Count by Day",
      x = "Minutes",
      y = "Activity Count"
    )
```

According to the graph, we can see that the patient is generally more active during the night around 8pm and during the morning/noon around 10am to 12pm. The patient has relatively low activity count during the approximate time range from 11pm to 6am, which makes sense because this is the sleeping time. There is no apparent difference among the activity count between days of the week, but there is a trend that demonstrates difference among the activity count between time of the day.



# Problem 3
## Load the NY NOAA dataset
```{r}
library(p8105.datasets)
data("ny_noaa")

head(ny_noaa)
```

The NY NOAA dataset has `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. Some important variables that provide weather-related information in the dataset include: `id` for the weather station ID, `date` for the observation date (including year, month, and day), `prcp` for the precipitation in mm, `snow` for the snowfall in mm, `snwd` for the snow depth in mm, `tmax` for the maximum temperature in celsius, and `tmin` for the minimum temperature in celsius.

Missing data is a big issue. There are many missing values in the dataset for the weather-related variables, especially for maximum and minimum temperatures (tmax and tmin).


## Clean the NY NOAA dataset
```{r}
ny_noaa = ny_noaa %>%
          separate(date, into = c("year", "month", "day"), convert = TRUE) %>%
        mutate(
          prcp = as.numeric(prcp),
          tmax = as.numeric(tmax), 
          tmin = as.numeric(tmin),
          prcp = prcp/10, 
          tmax = tmax/10, 
          tmin = tmin/10,
          year = as.factor(year),
          month = as.integer(month),
          day = as.integer(day)
        ) 

head(ny_noaa)
```

## What are the most commonly observed values for snowfall?
```{r}
snow = ny_noaa %>%
       count(snow) %>%
       arrange(desc(n))

snow
```

The most commonly observed values for snowfall is 0 with a count of observation as 2008508, because there is no snow observed by weather stations in most of the days. 


## Make a two-panel plot showing the average max temperature in January and in July in each station across years. 
```{r}
ny_noaa %>% 
  filter(month %in% c("1", "7")) %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax, color = month)) +
  geom_point(stat="identity",show.legend = F) + 
  facet_grid(.~month) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + 
  labs(title = "Average max temperature in January and in July in each station across years", x = "Year", y = "Average Max Temp in Celsius"
  )
```

According to the graph, the average maximum temperature in Celsius in January is always lower than the average maximum temperature in July across year. The average maximum temperature in January is usually between -10 and 10 Celsius, and the average maximum temperature in July is usually between 20 and 30 Celsius. The variance/difference among average maximum temperatures across year within January is larger than that within July. There are a couple of outliers in the graphs, such as July in year 1988, which had a temperature that's extremely low (around 14 Celsius).


## Make a two-panel plot showing the tmax and tmin for the full dataset, and the distribution of snowfall values greater than 0 and less than 100 separately by year. 
```{r}
library(patchwork)

tmax_tmin = 
  ny_noaa %>%
  ggplot(aes(x = as.numeric(tmax), y = as.numeric(tmin), color = month)) +
  geom_point(alpha = 0.5) +
  theme(legend.position = "none") +
  labs(
    x = "Maximum temperature in Celsius", 
    y = "Minimum temperature in Celsius"
    )

snow_dist = 
  ny_noaa %>%
  filter(snow > 0 & snow < 100) %>%
  ggplot(aes(x = year, y = snow, fill = year)) + 
  geom_violin(alpha = 0.5) + 
  theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust = 1)) + 
  theme(legend.position = "none") +
  labs(
    x = "Year", 
    y = "Snowfall in mm"
    )
 
tmax_tmin + snow_dist
```

According to the tmax_tmin graph, we can see that most of the tmax and tmin values are located around the center (around 0 Celsius), despite that there are a few outliers that demonstrate high level of variability. As shown on the graph, a small value of maximum temperature usually corresponds to a small value of minimum temperature, and vice versa.

According to the snow_dist graph, we can see that most stations have the snowfall observation value falls in the approximate range of 0 and 35mm in  most years.


